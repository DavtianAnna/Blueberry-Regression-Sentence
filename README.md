# ğŸ« Blueberry Yield Prediction - Kaggle Regression Challenge

This repository contains my solution for the [Blueberry Regression](https://www.kaggle.com/competitions/blueberry-regression-sentence/overview) Kaggle competition.  
The goal of this challenge is to accurately predict the **blueberry yield** (`yield`) based on environmental and agricultural data.  
It is a **regression task**, where the target is a continuous variable.


![uAWeWfDQVrFPdCgKmgZhX](https://github.com/user-attachments/assets/1452b282-4978-4a67-bc24-6eede040d9f2)


---

## ğŸ—‚ï¸ Project Structure

```

project-root/
â”œâ”€â”€ ğŸ“„ Blueberry.ipynb           # Main notebook: full ML pipeline from preprocessing to submission.
â”œâ”€â”€ ğŸ“Š train.csv                 # Training dataset with features and target ('yield').
â”œâ”€â”€ ğŸ§ª test.csv                  # Test dataset: features for prediction.
â”œâ”€â”€ ğŸ“ sample_submission.csv     # Template for Kaggle submission.
â”œâ”€â”€ ğŸš€ sub.csv                   # Submission from SGDRegressor.
â”œâ”€â”€ ğŸš€ sub1.csv                  # Submission from RandomForestRegressor.
â”œâ”€â”€ ğŸš€ sub2.csv                  # Submission from XGBRegressor.
â””â”€â”€ ğŸ“œ README.md                 # Project documentation.

```


---

## ğŸ’» Technologies Used

- **Language**: Python 3.x  
- **Environment**: Jupyter Notebook  

### ğŸ“¦ Libraries

- `pandas`, `numpy` â€“ Data manipulation  
- `matplotlib`, `seaborn` â€“ Visualization  
- `scikit-learn` â€“ Preprocessing & modeling (SGD, RF, pipelines, scaling)  
- `xgboost` â€“ XGBoost Regressor  
- `GridSearchCV` â€“ Hyperparameter tuning  
- `StandardScaler` â€“ Normalization  
- `mean_absolute_error`, `mean_squared_error` â€“ Model evaluation

---

## ğŸ“Š Dataset Description

Competition page: [Blueberry Regression](https://www.kaggle.com/competitions/blueberry-regression-sentence/overview)

- `train.csv`: Training data with the target variable `yield`
- `test.csv`: Unlabeled data for prediction
- `sample_submission.csv`: Required submission format

---

## ğŸ” Workflow Summary

The `Blueberry.ipynb` notebook follows the full ML workflow:

### 1. Data Loading & Preprocessing
- Load `train.csv` and `test.csv`
- Drop the `id` column
- Apply `StandardScaler` to numerical features

### 2. Modeling
Three regression models were built and compared:

| Model Name             | Methodology             | Output File |
|------------------------|--------------------------|-------------|
| **SGD Regressor**      | Pipeline + GridSearchCV  | `sub.csv`   |
| **RandomForest**       | GridSearchCV             | `sub1.csv`  |
| **XGBoost**            | GridSearchCV             | `sub2.csv`  |

All models were evaluated using:
- **MAE** (Mean Absolute Error)  
- **RMSE** (Root Mean Squared Error)

### 3. Prediction & Submission
- Final predictions on the test set were saved as:
  - `sub.csv` â†’ SGD Regressor
  - `sub1.csv` â†’ Random Forest Regressor
  - `sub2.csv` â†’ XGBoost Regressor

---

## ğŸ“ˆ Results Summary

| Model                  | Tuning Method         | Evaluation | Submission File  |    MAE   |   RMSE   |
|------------------------|-----------------------|------------|------------------|----------|----------|
| SGD Regressor          | GridSearchCV          | MAE / RMSE | `sub.csv`        |  269.94  |  388.11  |
| RandomForestRegressor  | GridSearchCV          | MAE / RMSE | `sub1.csv`       |  257.78  |  374.73  |
| XGBRegressor           | GridSearchCV          | MAE / RMSE | `sub2.csv`       |  257.40  |  373.27   |
 
---

## âš™ï¸ Installation

To install all required libraries:

```bash
pip install pandas numpy matplotlib seaborn scikit-learn xgboost
